{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING AN END TO END SOLUTION: Section 4\n",
    "## Loan Application\n",
    "\n",
    "Presented by Shaun\n",
    "\n",
    "Throughout the financial sector, machine learning algorithms are being developed to approve loan applications. In this project,we will process a dataset and use techniques to construct three machine learning models to predict on loan approvals. A dataset with 1102 observations will be examined, cleaned and organised; a data exploration will identify the features and their data types and the most important features, which are the best predictors for loan approvals.\n",
    "\n",
    "### Model Persistence \n",
    "*The reason models are persisted is because, large and complex datasets may take days to train; therefore, it is not efficient, to retrain a model each time we want to evaluate new data against the classifier.  To persist the model, we will use a library from scikit-learn called **Pickle** which is the standard way in Python of serializing machine learning models, which will save the model in a serialized format to a file. Once the model is serialized as a file, this can then be loaded and deserialized to classify new data presented to it. The difference between the three classifiers when using Pickle to persist the model, is that the k-nearest neighbour algorithm stores the entire dataset to file, which could pose a problem on very large datasets.* \n",
    "\n",
    "#### Final Model\n",
    "We are at the stage where we now need to create the **final model.** This is the model that will be used to make predictions on new data. In our project the final model will be trained on the full dataset. **Note:** There is alot of debate how the final model should be trained but, as this is a beginner tutorial we will keep it simple, but please review the research and discussions in this area. \n",
    "\n",
    "**Pickle:**  The pickle module implements a fundamental, but powerful algorithm for serializing and de-serializing a Python object structure. More details here:  __[Pickle](https://docs.python.org/3.1/library/pickle.html)__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "##impoting the classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "##import Pickle to serialize the machine learning models.\n",
    "import pickle\n",
    "\n",
    "##import the train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##scoring metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score, f1_score,roc_curve, auc\n",
    " \n",
    "##Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predata = pd.read_csv('c:\\\\ml\\\\LoanPredOrig.csv')\n",
    "predata = predata.dropna()\n",
    "#Categorical data clean up, this will error if you try to run this twice consecutively, without reloading the dataset\n",
    "predata['Loan_Status'] = predata.Loan_Status.astype(int)\n",
    "predata['Employed'] = np.where(predata['Employed'].str.contains('YES'), 1, 0)\n",
    "predata['Marital_Status'] = np.where(predata['Marital_Status'].str.contains('YES'), 1, 0)\n",
    "predata['Graduate'] = np.where(predata['Graduate'].str.contains('YES'), 1, 0)\n",
    "predata['Credit_History'] = np.where(predata['Credit_History'].str.contains('YES'), 1, 0)\n",
    "predata['PropertyOwner'] = np.where(predata['PropertyOwner'].str.contains('YES'), 1, 0)\n",
    "df = pd.DataFrame(predata)\n",
    "\n",
    "#Drop the Loanid column\n",
    "del df['Loanid']\n",
    "\n",
    "#One hot encding on the Gender column\n",
    "data = pd.get_dummies(df,columns=['Gender'])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the classifier you want to pickle\n",
    "*Here we specify the classsifier and the filename of the final model* <BR>\n",
    "**Note:** We have kept the code simple here, feel free to rewrite this if you want, and use a loop for this process.<BR>\n",
    "*Un-Hash # each model you want to pickle *\n",
    "**Note:** *In our exmple we have the Naive Bayes selected 'NB', this model was clearly the best classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "name ='NB'\n",
    "#name ='KNN'\n",
    "#name ='ANN'\n",
    "if name =='KNN':\n",
    "    clf = KNeighborsClassifier()\n",
    "    filename = 'KNNClassifier.pkl'\n",
    "if name =='NB':\n",
    "    clf =GaussianNB()\n",
    "    filename = 'NBGauClassifier.pkl'\n",
    "if name =='ANN':\n",
    "    clf = MLPClassifier() ## add the optimal parameters here for your MLP model otherwise the default will be used\n",
    "    filename = 'ANNClassifier.pkl' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the final model\n",
    "**Note:** *Here we use the fit() method, to train the final model on the full dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data.loc[:, data.columns != 'Loan_Status'], data['Loan_Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the Model\n",
    "**To Pickle the model we need to:**\n",
    "1. *First we create a varaible, 'dumpmodelfile and assign it the open() function,  which opens the file for writing, this takes two arguments.*\n",
    "2. *The first argument is the name of our file, 'filename', we already assigned this a value in the 'Specify the classifier you want to pickle', section *\n",
    "3. *For second argument we specify 'wb'. The w means that we will be writing to the file, and the b refers to binary mode.*\n",
    "4. *Now we can dump the data using the pickle.dump() fuction, which takes two arguments. *\n",
    "5. *The first, is 'clf', which is the object we want to pickle.*\n",
    "6. *The second, the object that needs to be saved, the 'dumpmodelfile'.*\n",
    "7. *Lastly we close the file with the close() function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpmodelfile = open(filename,'wb')\n",
    "pickle.dump(clf,dumpmodelfile)\n",
    "dumpmodelfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnPickle the Model\n",
    "#### To UnPickle the model we need to:\n",
    "\n",
    "1. *First we create a varaible 'model_pkl' and assign it the open() function  which oppens the file for reading, this takes two arguments.*\n",
    "2. *The first argument is 'filename', we specified earlier.*\n",
    "3. *For second argument we specify 'rb'. The r means that we will be reading the file, and the b refers to binary mode,which is the way the file was saved in.*\n",
    "4. *Now we need to load the file, we use pickle.load(), with infile 'model_pkl', as the argument.*\n",
    "5. *Lastly we close the file with the close() function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pkl = open(filename, 'rb')\n",
    "FinalModel = pickle.load(model_pkl)\n",
    "model_pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the UnPickled Model.\n",
    "**Review the unpickled model using print and type().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the saved Final Model ::  GaussianNB(priors=None)\n",
      "Model Class Type <class 'sklearn.naive_bayes.GaussianNB'>\n"
     ]
    }
   ],
   "source": [
    "print (\"Loaded the saved Final Model :: \", FinalModel)\n",
    "print('Model Class Type',type(FinalModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model with new data\n",
    "We can test the model by providing it with data in 2 d arrays [[]], this is how the Machine Learning Model will expect to receive data.\n",
    "I have provided examples below of new_data1 with 1 set of data and new_data2 with 2 sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Final Model Prediction:  [1]\n",
      "Saved Final Model Prediction:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "#(Marital_Status,Dependents,Graduate,Employed,ApplicantIncome,CoapplicantIncome,LoanAmount,Loan_Amount_Term,Credit_History,PropertyOwner,Gender_Female,Gender_Male)\n",
    "## 1 set of data\n",
    "new_data1 =[[1,2,1,1,5000,1508,120,360,1,1,0,1]]\n",
    "## 2 sets of data\n",
    "new_data2 =[[0,7,0,0,0,0,120,360,0,0,1,0],[1,2,1,1,5000,1508,120,360,1,1,0,1]]\n",
    "\n",
    "\n",
    "FinalModelPred = FinalModel.predict(new_data1)\n",
    "print('Saved Final Model Prediction: ', FinalModelPred)\n",
    "\n",
    "FinalModelPred = FinalModel.predict(new_data2)\n",
    "print('Saved Final Model Prediction: ', FinalModelPred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congratualations! You have built a final machine learning model and have completed Section 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
